{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["lcWCr0V9vqIE"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0a0fa70f44714598ae06164ffa2e5b3a":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_44c41a8a9c614deb94042770198f4a34","IPY_MODEL_daeb666fd4924a3687303d66422f8b68","IPY_MODEL_e9fa089bc2594d29a208a15a36facfc9","IPY_MODEL_22b95b7ccd5541d79a4f87844bfe541e","IPY_MODEL_e93ef7017ff64b089e5cf0f7faf6ae78"],"layout":"IPY_MODEL_a1564204af814fb398ed1b4191c429f2"}},"44c41a8a9c614deb94042770198f4a34":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4401c35d92a484f940c202dc05b338a","placeholder":"​","style":"IPY_MODEL_cc1e0a84b6b147a88c51b40e5eaafcfc","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"daeb666fd4924a3687303d66422f8b68":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_ab66099383484634815c95b2e28aaf0b","placeholder":"​","style":"IPY_MODEL_e857006fe5a4419a906723b062696c26","value":""}},"e9fa089bc2594d29a208a15a36facfc9":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_85f353e6cfe144c5894feb076b3e11ad","style":"IPY_MODEL_246f052fcbd044459136a8c51eaab40b","value":true}},"22b95b7ccd5541d79a4f87844bfe541e":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_eb647556d83c448e8701be53d5bc65e4","style":"IPY_MODEL_c08dc4398efc488cb2b4b156cf57d0e8","tooltip":""}},"e93ef7017ff64b089e5cf0f7faf6ae78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53f5100bc9c746f388442f00cd417a1a","placeholder":"​","style":"IPY_MODEL_6a7afa09bb17443bbc3205ed9a430660","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"a1564204af814fb398ed1b4191c429f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"c4401c35d92a484f940c202dc05b338a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc1e0a84b6b147a88c51b40e5eaafcfc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab66099383484634815c95b2e28aaf0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e857006fe5a4419a906723b062696c26":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85f353e6cfe144c5894feb076b3e11ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"246f052fcbd044459136a8c51eaab40b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb647556d83c448e8701be53d5bc65e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c08dc4398efc488cb2b4b156cf57d0e8":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"53f5100bc9c746f388442f00cd417a1a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a7afa09bb17443bbc3205ed9a430660":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# 最終課題（メインコンペ） 推論標準コード\n","\n","## 1.概要\n","このノートブックは、あなたが学習して Hugging Face にアップした **LoRAアダプタ**を用いて、  \n","**ベンチマークの推論結果JSONを生成し、コンペの提出用JSONファイルを生成する**ための標準コードです。\n","- コンペでの提出物は **学習済みLoRAそのものではなく、推論結果のJSONファイル**です。  \n","- 本ノートは、その提出用JSONを確実に作るための手順を提供します。\n","\n","- StructEval-Tからサンプリングした150問の回答を生成（推論）します。\n","- 実行にあたっては、/content/public_150.json（配布資料）が必要です。\n","- 出力は**inference.json（提出形式）**ですので、これをOmniCampusにアップロードして採点してください。"],"metadata":{"id":"Yo1rCsGugXIw"}},{"cell_type":"markdown","source":["## 2. 事前準備"],"metadata":{"id":"i86bwRT4iM1E"}},{"cell_type":"markdown","source":["\n","- Colab のランタイムを **GPU（T4）** に設定してください。\n","- Hugging Face にログインします（トークン入力が必要です）。\n","- 推論に使う LoRA アダプタは、原則として「学習ノートでアップロードしたもの」を使用します。\n","\n","---"],"metadata":{"id":"P69owhiqiM3n"}},{"cell_type":"markdown","source":["\n","\n","## 3. 実行手順（推奨フロー）\n"],"metadata":{"id":"kK0KGIY3iM8N"}},{"cell_type":"markdown","source":["\n","### Step 0: セットアップ（clone / install）\n","上から順にセルを実行します。\n","\n","- `StructEval` を clone し、依存関係（vLLM等）を導入します。\n","- `python3 -m structeval.cli --help` が表示されれば、基本セットアップは成功です。\n","\n","### Step 1: Hugging Face ログイン\n","- `login()` を実行し、トークンを入力してください。\n","\n","### Step 2: LoRA の統合（merge）\n","- `adapter_id` にある LoRA を読み込み、ベースモデルと統合して `./merged_model` を生成します。\n","- ここが完了すると、以降の推論は **`./merged_model` をモデルパスとして使用**します。\n","\n","### Step 3: vLLM 推論の実行と提出用JSONの生成\n","- `custom_inference.py` が生成され、それを実行します。\n","- 推論結果は `/content/StructEval/outputs/nonrenderable.json` に保存されます。\n","- `output` を `generation` に補完し、提出用ファイル `/content/inference.json` を出力します。\n","- 出力された`/content/inference.json` をダウンロードして、Omnicampusに提出してください。\n","---"],"metadata":{"id":"awzqJJPIiM-_"}},{"cell_type":"markdown","source":["\n","\n","## 4. 出力ファイル（提出物）の扱い\n"],"metadata":{"id":"CnpX8ZJ7gzrF"}},{"cell_type":"markdown","source":["\n","### 4.1 生成される主なファイル\n","- 統合済みモデル（提出不要）\n","  - `./merged_model/`\n","\n","- 推論結果 **提出用ファイル（最重要）**\n","  - `/content/inference.json`\n","  - ※このファイルは `generation` フィールドを持つ形式に整形済みです。\n","\n","### 4.2 提出手順（ダウンロード → Omnicampus にアップロード）\n","1. Colab 上で、最終成果物 `/content/inference.json` をローカルPCに **ダウンロード**します。\n","   - Colab 左の「Files（フォルダアイコン）」から `/content/` を開く\n","   - `inference.json` を右クリック → **Download**\n","\n","2. Omnicampus の提出画面で、ダウンロードした `inference.json` を **アップロードして提出**します。\n","\n","提出ファイル名は、`inference.json` としてください。\n","\n","\n","### **4.3 コンペ参加における注意点**：\n","- 本コードによる推論には「学習してアップしたLoRA」を使ってください。それ以外のモデルを使った推論結果を提出した方は、失格となります。\n","- 提出物は「推論結果 JSON」です（LoRA自体の提出ではありません）。\n","- 提出の際に、HuggingFaceにアップしたアダプタのURLを必ず記載してください。\n","\n","---"],"metadata":{"id":"Dzu1fQyiiyL5"}},{"cell_type":"markdown","source":["## 5. よくある失敗と対策\n"],"metadata":{"id":"YBkvMWDCiyO8"}},{"cell_type":"markdown","source":["\n","- **GPUが有効になっていない**\n","  - 推論が極端に遅い／vLLMが動かない原因になります。必ず T4 を確認してください。\n","\n","- **`./merged_model` が存在しない**\n","  - LoRA統合（merge）が完了していない可能性があります。mergeセルを再実行してください。\n","\n","- **vLLM 実行時に OOM（Out of Memory）になる**\n","  - 本標準コードは `gpu_memory_utilization=0.6` で安全寄りですが、環境差で落ちる場合があります。\n","  - その場合は、まずランタイム再起動（Factory reset）→同じ手順で再実行してください。\n","\n","\n","\n","---\n"],"metadata":{"id":"KDHi4u0ggztj"}},{"cell_type":"markdown","source":["\n","\n","## 6. 期待する最終状態（チェック）\n"],"metadata":{"id":"uGKq9afckmbT"}},{"cell_type":"markdown","source":["\n","提出直前に、次を満たしていればOKです。\n","\n","- `/content/inference.json` が存在する\n","- そのJSONが list であり、各要素に `generation` フィールドが入っている（空でない）\n","- Omnicampus に `inference.json` をアップロードして提出\n","---\n"],"metadata":{"id":"zMcE5FIVkmd9"}},{"cell_type":"markdown","source":["# 実行コード"],"metadata":{"id":"2LAE32yBP4Pr"}},{"cell_type":"markdown","source":["\n","### Step 0: セットアップ（clone / install）"],"metadata":{"id":"WJ8okWTTviHJ"}},{"cell_type":"code","source":["# 0) Setup (バージョン固定)\n","\n","!git clone -b fix-module-not-found-issue-2 https://github.com/Osakana7777777/StructEval.git\n","\n","!uv pip install \\\n","  \"vllm==0.13.0\" \\\n","  \"torch==2.9.0\" \\\n","  \"torchaudio==2.9.0\" \\\n","  \"torchvision==0.24.0\" \\\n","  \"triton==3.5.0\" \\\n","  \"compressed-tensors==0.12.2\" \\\n","  \"openai==2.15.0\" \\\n","  \"xgrammar==0.1.27\" \\\n","  \"bitsandbytes==0.46.1\" \\\n","  fire\n","\n","# flash-attn だけは環境によって挙動が変わるためバージョン固定しない\n","!uv pip install flash-attn --no-build-isolation\n","\n","%cd StructEval\n","!uv pip install -e .\n","\n","!python3 -m structeval.cli --help\n","!mkdir -p outputs\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UGIUuwBZz9-o","executionInfo":{"status":"ok","timestamp":1769245343668,"user_tz":-540,"elapsed":97292,"user":{"displayName":"daichi t","userId":"03473696301622666053"}},"outputId":"e04f8c0d-8153-42fd-8bf1-547e4aad4099","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'StructEval'...\n","remote: Enumerating objects: 17398, done.\u001b[K\n","remote: Counting objects: 100% (149/149), done.\u001b[K\n","remote: Compressing objects: 100% (123/123), done.\u001b[K\n","remote: Total 17398 (delta 91), reused 45 (delta 26), pack-reused 17249 (from 3)\u001b[K\n","Receiving objects: 100% (17398/17398), 529.90 MiB | 16.40 MiB/s, done.\n","Resolving deltas: 100% (5424/5424), done.\n","\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2K\u001b[2mResolved \u001b[1m165 packages\u001b[0m \u001b[2min 1.92s\u001b[0m\u001b[0m\n","\u001b[2K\u001b[2mPrepared \u001b[1m47 packages\u001b[0m \u001b[2min 20.14s\u001b[0m\u001b[0m\n","\u001b[2mUninstalled \u001b[1m4 packages\u001b[0m \u001b[2min 118ms\u001b[0m\u001b[0m\n","\u001b[2K\u001b[2mInstalled \u001b[1m47 packages\u001b[0m \u001b[2min 172ms\u001b[0m\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1manthropic\u001b[0m\u001b[2m==0.71.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mapache-tvm-ffi\u001b[0m\u001b[2m==0.1.8.post2\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mastor\u001b[0m\u001b[2m==0.8.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mbitsandbytes\u001b[0m\u001b[2m==0.46.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mblake3\u001b[0m\u001b[2m==1.0.8\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mcbor2\u001b[0m\u001b[2m==5.8.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mcompressed-tensors\u001b[0m\u001b[2m==0.12.2\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mdepyf\u001b[0m\u001b[2m==0.20.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mdnspython\u001b[0m\u001b[2m==2.8.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1memail-validator\u001b[0m\u001b[2m==2.3.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mfastapi-cli\u001b[0m\u001b[2m==0.0.20\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mfastapi-cloud-cli\u001b[0m\u001b[2m==0.11.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mfastar\u001b[0m\u001b[2m==0.8.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mfire\u001b[0m\u001b[2m==0.7.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mflashinfer-python\u001b[0m\u001b[2m==0.5.3\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mgguf\u001b[0m\u001b[2m==0.17.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mijson\u001b[0m\u001b[2m==3.4.0.post0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1minteregular\u001b[0m\u001b[2m==0.3.3\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mjmespath\u001b[0m\u001b[2m==1.1.0\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.3.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mllguidance\u001b[0m\u001b[2m==1.3.0\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.43.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.44.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mlm-format-enforcer\u001b[0m\u001b[2m==0.11.3\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mloguru\u001b[0m\u001b[2m==0.7.3\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mmistral-common\u001b[0m\u001b[2m==1.8.8\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mmodel-hosting-container-standards\u001b[0m\u001b[2m==0.1.13\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.20.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mninja\u001b[0m\u001b[2m==1.13.0\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.60.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.61.2\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-frontend\u001b[0m\u001b[2m==1.17.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mnvidia-cutlass-dsl\u001b[0m\u001b[2m==4.3.5\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mopenai-harmony\u001b[0m\u001b[2m==0.0.8\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1moutlines-core\u001b[0m\u001b[2m==0.2.11\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mpartial-json-parser\u001b[0m\u001b[2m==0.2.1.1.post7\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mprometheus-fastapi-instrumentator\u001b[0m\u001b[2m==7.1.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mpybase64\u001b[0m\u001b[2m==1.4.3\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mpycountry\u001b[0m\u001b[2m==24.6.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mpydantic-extra-types\u001b[0m\u001b[2m==2.11.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mray\u001b[0m\u001b[2m==2.53.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mrich-toolkit\u001b[0m\u001b[2m==0.17.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mrignore\u001b[0m\u001b[2m==0.7.6\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1msetproctitle\u001b[0m\u001b[2m==1.3.7\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==75.2.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==80.10.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1msupervisor\u001b[0m\u001b[2m==4.3.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mvllm\u001b[0m\u001b[2m==0.13.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mxgrammar\u001b[0m\u001b[2m==0.1.27\u001b[0m\n","\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2K\u001b[2mResolved \u001b[1m28 packages\u001b[0m \u001b[2min 1.05s\u001b[0m\u001b[0m\n","\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 14.14s\u001b[0m\u001b[0m\n","\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mflash-attn\u001b[0m\u001b[2m==2.8.3\u001b[0m\n","/content/StructEval\n","\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2K\u001b[2mResolved \u001b[1m235 packages\u001b[0m \u001b[2min 2.54s\u001b[0m\u001b[0m\n","\u001b[2K\u001b[2mPrepared \u001b[1m38 packages\u001b[0m \u001b[2min 1.19s\u001b[0m\u001b[0m\n","\u001b[2mUninstalled \u001b[1m11 packages\u001b[0m \u001b[2min 33ms\u001b[0m\u001b[0m\n","\u001b[2K\u001b[2mInstalled \u001b[1m38 packages\u001b[0m \u001b[2min 48ms\u001b[0m\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1maiodns\u001b[0m\u001b[2m==4.0.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1masttokens\u001b[0m\u001b[2m==3.0.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1masyncstdlib-fw\u001b[0m\u001b[2m==3.13.2\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==25.4.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==23.2.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mbackports-zstd\u001b[0m\u001b[2m==1.3.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mbetterproto-fw\u001b[0m\u001b[2m==2.0.3\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mblack\u001b[0m\u001b[2m==25.12.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mcolorama\u001b[0m\u001b[2m==0.4.6\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mdotenv\u001b[0m\u001b[2m==0.9.9\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1meval-type-backport\u001b[0m\u001b[2m==0.2.2\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mexecuting\u001b[0m\u001b[2m==2.2.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mfireworks-ai\u001b[0m\u001b[2m==0.19.20\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mhttpx-ws\u001b[0m\u001b[2m==0.8.2\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1micecream\u001b[0m\u001b[2m==2.1.10\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1minvoke\u001b[0m\u001b[2m==2.2.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mllm-engines\u001b[0m\u001b[2m==0.0.25\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mmistralai\u001b[0m\u001b[2m==1.10.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mmypy-extensions\u001b[0m\u001b[2m==1.1.0\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.37.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.38.0\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-common\u001b[0m\u001b[2m==1.37.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-common\u001b[0m\u001b[2m==1.38.0\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-http\u001b[0m\u001b[2m==1.37.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-http\u001b[0m\u001b[2m==1.38.0\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mopentelemetry-proto\u001b[0m\u001b[2m==1.37.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mopentelemetry-proto\u001b[0m\u001b[2m==1.38.0\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.37.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.38.0\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.58b0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.59b0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mpathspec\u001b[0m\u001b[2m==1.0.3\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mpdf2image\u001b[0m\u001b[2m==1.17.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mplaywright\u001b[0m\u001b[2m==1.57.0\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==5.29.5\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==5.29.3\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mpycares\u001b[0m\u001b[2m==5.0.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mpyee\u001b[0m\u001b[2m==13.0.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mpytokens\u001b[0m\u001b[2m==0.4.0\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==13.9.4\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==14.2.0\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mruff\u001b[0m\u001b[2m==0.14.13\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mruff\u001b[0m\u001b[2m==0.9.10\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mstructeval\u001b[0m\u001b[2m==0.0.5 (from file:///content/StructEval)\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mtogether\u001b[0m\u001b[2m==1.5.35\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.21.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.19.2\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mwsproto\u001b[0m\u001b[2m==1.3.2\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mxmltodict\u001b[0m\u001b[2m==1.0.2\u001b[0m\n","INFO: Showing help with the command 'cli.py -- --help'.\n","\n","\u001b[1mNAME\u001b[0m\n","    cli.py\n","\n","\u001b[1mSYNOPSIS\u001b[0m\n","    cli.py -\n"]}]},{"cell_type":"markdown","source":["\n","### Step 1: Hugging Face ログイン\n","- `login()` を実行し、トークンを入力してください。\n"],"metadata":{"id":"lcWCr0V9vqIE"}},{"cell_type":"code","source":["\n","# -----------------------------\n","# 1) HF login (once)\n","# -----------------------------\n","# HF Hub上のデータセットを読むため、HuggingFaceにログインします。\n","#\n","from huggingface_hub import login\n","login()  # Colab will prompt"],"metadata":{"id":"SrjUvUoPP6i9","colab":{"base_uri":"https://localhost:8080/","height":483,"referenced_widgets":["0a0fa70f44714598ae06164ffa2e5b3a","44c41a8a9c614deb94042770198f4a34","daeb666fd4924a3687303d66422f8b68","e9fa089bc2594d29a208a15a36facfc9","22b95b7ccd5541d79a4f87844bfe541e","e93ef7017ff64b089e5cf0f7faf6ae78","a1564204af814fb398ed1b4191c429f2","c4401c35d92a484f940c202dc05b338a","cc1e0a84b6b147a88c51b40e5eaafcfc","ab66099383484634815c95b2e28aaf0b","e857006fe5a4419a906723b062696c26","85f353e6cfe144c5894feb076b3e11ad","246f052fcbd044459136a8c51eaab40b","eb647556d83c448e8701be53d5bc65e4","c08dc4398efc488cb2b4b156cf57d0e8","53f5100bc9c746f388442f00cd417a1a","6a7afa09bb17443bbc3205ed9a430660"]},"outputId":"50eaf971-9d14-4255-a179-4ee8924997d6","executionInfo":{"status":"ok","timestamp":1769245343882,"user_tz":-540,"elapsed":149,"user":{"displayName":"daichi t","userId":"03473696301622666053"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a0fa70f44714598ae06164ffa2e5b3a"}},"metadata":{}}]},{"cell_type":"markdown","source":["\n","### Step 2: LoRA の統合（merge）\n","- `adapter_id` にある LoRA を読み込み、ベースモデルと統合して `./merged_model` を生成します。\n","- ここが完了すると、以降の推論は **`./merged_model` をモデルパスとして使用**します。\n","\n"],"metadata":{"id":"P8fExEKkwEtn"}},{"cell_type":"markdown","source":["- ここで、contentフォルダに\"public_150.json\"をアップロードしてください。\n","- Colabのファイル領域(/content)に、評価用の public_150.json を置く必要があります。"],"metadata":{"id":"xTCIiMYqe87c"}},{"cell_type":"code","source":["# ------------------------------------------------------------\n","# 1) Config\n","# ------------------------------------------------------------\n","\n","MODEL_SOURCE = \"adapter_merge\"   # \"merged\" | \"base\" | \"adapter_merge\"\n","# どのモデルを使うかを選びます。今回は、基本的に\"adapter_merge\"を選んでください。\n","\n","#   - \"base\"        : ベースモデル（学習していない素のモデル）\n","#   - \"merged\"      : すでにLoRAをマージ済みのモデル（完成品として配布されている想定）\n","#   - \"adapter_merge\": ベースモデル + LoRAアダプタをその場で読み込み、ローカルでマージしてから使う\n","\n","# base model (HF repo id or local path)\n","# 学習時に使用したベースモデルを入れてください。\n","BASE_MODEL_ID_OR_PATH   = \"Qwen/Qwen3-4B-Instruct-2507\"\n","\n","# merged model (HF repo id or local path)\n","# アダプタではなくマージモデルをアップロードした場合は、ここにIDをいれてください。\n","# \"merged\"を選択した場合に記入\n","MERGED_MODEL_ID_OR_PATH = \"your_id/your-merged-repo\"\n","\n","# adapter merge\n","# あなたがHuggingFaceにアップロードしたアダプタのIDを入れてください。\n","# \"adapter_merge\"を選択した場合に記入\n","ADAPTER_ID       = \"your_id/test-lora-repo\"\n","\n","# merge済モデルの一時保存\n","MERGED_LOCAL_DIR = \"./merged_model\"\n","\n","# 入力（150問）と出力（提出用）ファイルパスの指定\n","INPUT_PATH  = \"/content/public_150.json\"\n","OUTPUT_PATH = \"/content/inference.json\"\n","\n","\n","TEMPERATURE = 0.0\n","#   0.0 は最も決定的（同じ入力なら同じ出力になりやすい）で、評価用途では一般に安定します。\n"],"metadata":{"id":"5_F2mbjIwRoE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 3: vLLM 推論の実行と提出用JSONの生成\n","- `custom_inference.py` が生成され、それを実行します。\n","- 推論結果は `/content/StructEval/outputs/nonrenderable.json` に保存されます。\n","- `output` を `generation` に補完し、提出用ファイル `/content/inference.json` を出力します。\n","- 出力された`/content/inference.json` をダウンロードして、Omnicampusに提出してください。\n","---"],"metadata":{"id":"-O-8HtLKwRx5"}},{"cell_type":"code","source":["\n","# ------------------------------------------------------------\n","# 2) Stable vLLM env (IMPORTANT: must be set BEFORE importing vllm)\n","# ------------------------------------------------------------\n","\n","import os\n","os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n","# vLLM内部でワーカープロセスを作る方式を \"spawn\" に固定します。\n","# Colabなど一部環境では \"fork\" より安定しやすいことがあります。\n","\n","os.environ[\"VLLM_LOGGING_LEVEL\"] = \"INFO\"\n","# vLLMのログレベル（INFO）を設定します。デバッグ時に有用です。\n","\n","# ------------------------------------------------------------\n","# 3) Resolve model_path\n","# ------------------------------------------------------------\n","# 選んだMODEL_SOURCEに応じて、最終的にvLLMに渡す「モデルの場所(model_path)」を決めます。\n","\n","def resolve_model_path():\n","    # どのモデルを使うかに応じて、vLLMへ渡すパス/IDを返す関数\n","\n","    if MODEL_SOURCE == \"base\":\n","        return BASE_MODEL_ID_OR_PATH\n","\n","    if MODEL_SOURCE == \"merged\":\n","        return MERGED_MODEL_ID_OR_PATH\n","\n","    if MODEL_SOURCE == \"adapter_merge\":\n","        # NOTE: torch/CUDA（GPU）を触るため、vLLMを起動する前に済ませます。\n","        import os, gc\n","        import torch\n","        from transformers import AutoModelForCausalLM, AutoTokenizer\n","        from peft import PeftModel\n","        print(\"[INFO] Merging adapter into base model...\")\n","        base_model = AutoModelForCausalLM.from_pretrained(\n","            BASE_MODEL_ID_OR_PATH,\n","            dtype=torch.float16,\n","            device_map=\"auto\",\n","            trust_remote_code=True,\n","        )\n","        # ベースモデルに対応するトークナイザを読み込み（マージ後も同じものを使うのが通常）\n","        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID_OR_PATH, trust_remote_code=True)\n","\n","        # base_model に LoRAアダプタ(ADAPTER_ID) をマージ\n","        # merge後はLoRA層を外せるので（unload）、推論時の扱いが単純になります。\n","        model_to_merge = PeftModel.from_pretrained(base_model, ADAPTER_ID)\n","        merged_model = model_to_merge.merge_and_unload()\n","\n","        os.makedirs(MERGED_LOCAL_DIR, exist_ok=True)\n","        merged_model.save_pretrained(MERGED_LOCAL_DIR)\n","        tokenizer.save_pretrained(MERGED_LOCAL_DIR)\n","\n","        del base_model, model_to_merge, merged_model\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        print(\"[INFO] Merged model saved:\", MERGED_LOCAL_DIR)\n","        return MERGED_LOCAL_DIR\n","\n","    raise ValueError(\"MODEL_SOURCE must be 'merged'|'base'|'adapter_merge'\")\n","\n","# 最終的に使うモデルのパス/IDを確定\n","model_path = resolve_model_path()\n","print(\"[INFO] Using model:\", model_path)\n","\n","# ------------------------------------------------------------\n","# 4) Load public_150 and build prompts (no torch usage here)\n","# ------------------------------------------------------------\n","# 入力ファイルを読み込み、各問題の「プロンプト（モデルに渡す文字列）」を作ります。\n","\n","import json\n","from pathlib import Path\n","from transformers import AutoTokenizer\n","\n","pub = json.loads(Path(INPUT_PATH).read_text(encoding=\"utf-8\"))\n","\n","assert isinstance(pub, list), \"public_150.json must be a list\"\n","assert len(pub) == 150, f\"public_150 must have 150 items, got {len(pub)}\"\n","assert len({x[\"task_id\"] for x in pub}) == 150, \"public_150 has duplicate task_id\"\n","\n","# Safety: ensure output_type exists (office enriched file)\n","\n","missing_ot = [x.get(\"task_id\") for x in pub if not (x.get(\"output_type\") or \"\").strip()]\n","\n","if missing_ot:\n","    raise RuntimeError(f\"FATAL: public_150 missing output_type (not enriched). Examples: {missing_ot[:5]}\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n","\n","# task_ids: 出力に使う task_id の並びを保存\n","# prompts:   vLLMに渡すプロンプト文字列を保存\n","task_ids, prompts = [], []\n","\n","for item in pub:\n","    task_ids.append(item[\"task_id\"])\n","    query = item.get(\"query\", \"\")\n","    messages = [{\"role\": \"user\", \"content\": query}]\n","    prompts.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n","    # ↑ apply_chat_template で「モデルが期待する会話形式の文字列」に整形\n","    #   tokenize=False : まだトークン化せず、文字列として返す\n","    #   add_generation_prompt=True : 「ここからアシスタントが答える」境界を追加\n","    #   これにより、モデルが回答を続けて生成しやすい形になります。\n","\n","# ------------------------------------------------------------\n","# 5) Presets + fallback plan\n","# ------------------------------------------------------------\n","# vLLM起動時に「文脈長(max_model_len)」や「出力上限(max_tokens)」を大きくしすぎると、\n","# GPUメモリ不足(OOM)で落ちやすいです。\n","# そこで、成功しやすい設定をいくつか用意し、失敗したら段階的に軽くして再試行します。\n","# merged（既に焼き込み済み）と adapter_merge（その場でマージ）では、\n","# 実メモリ使用量が変わることがあるため、最初に試す設定（gpu_memなど）を変えています。\n","# 事前に「試行候補リスト」を作り、上から順に試します。\n","\n","def build_try_configs():\n","\n","    # Primary presets\n","\n","    if MODEL_SOURCE == \"merged\":\n","        base = [\n","            {\"max_model_len\": 4096, \"max_tokens\": 4096, \"gpu_mem\": 0.85},\n","            {\"max_model_len\": 4096, \"max_tokens\": 4096, \"gpu_mem\": 0.80},\n","        ]\n","        # ↑ 4096トークンまでの文脈/出力を許しつつ、GPU使用率を0.85→0.80で試す\n","\n","    elif MODEL_SOURCE == \"adapter_merge\":\n","        base = [\n","            {\"max_model_len\": 4096, \"max_tokens\": 4096, \"gpu_mem\": 0.60},\n","            {\"max_model_len\": 4096, \"max_tokens\": 4096, \"gpu_mem\": 0.65},\n","        ]\n","        # ↑ adapter_merge はメモリが厳しくなりがちなので、gpu_memを低めから試します。\n","\n","    else:  # base\n","        base = [\n","            {\"max_model_len\": 4096, \"max_tokens\": 4096, \"gpu_mem\": 0.80},\n","            {\"max_model_len\": 4096, \"max_tokens\": 4096, \"gpu_mem\": 0.70},\n","        ]\n","        # ↑ baseモデルは比較的軽い想定で、0.80→0.70を試します。\n","\n","    # Fallback ladder (reduce context / output)\n","    # 失敗したときの「段階的に軽くする設定」。\n","    # max_model_len と max_tokens を下げると、必要メモリが減り成功しやすくなります。\n","    ladder = [\n","        {\"max_model_len\": 3072, \"max_tokens\": 3072},\n","        {\"max_model_len\": 2048, \"max_tokens\": 2048},\n","        {\"max_model_len\": 1536, \"max_tokens\": 1536},\n","    ]\n","\n","    # Expand base configs with ladder and a couple gpu_mem tweaks\n","    # ↑ base設定に対し、ladder段階を「合成」して試行パターンを増やします。\n","    #   また、gpu_memも少し増やす版を試します（失敗理由が「確保不足」系のときに効く場合がある）。\n","    out = []\n","    for cfg in base:\n","        out.append(cfg)\n","\n","        for step in ladder:\n","            out.append({**cfg, **step})\n","\n","        # try a slightly higher gpu_mem if still failing (some failures are \"not enough alloc\")\n","        out.append({**cfg, \"gpu_mem\": min(0.90, cfg[\"gpu_mem\"] + 0.05)})\n","\n","    # Deduplicate while preserving order\n","    # ↑ 似た設定が重複し得るので、順序を保ったまま重複削除します。\n","    seen = set()\n","    uniq = []\n","    for c in out:\n","        key = (c[\"max_model_len\"], c[\"max_tokens\"], round(c[\"gpu_mem\"], 2))\n","\n","        if key in seen:\n","            continue\n","\n","        seen.add(key)\n","        uniq.append(c)\n","\n","    return uniq\n","\n","\n","TRY_CONFIGS = build_try_configs()\n","# ↑ 実際に試す設定リストを作成します。\n","\n","print(\"[INFO] Try configs (in order):\")\n","\n","for i, c in enumerate(TRY_CONFIGS[:8], 1):\n","    print(f\"  {i:02d}. max_model_len={c['max_model_len']} max_tokens={c['max_tokens']} gpu_mem={c['gpu_mem']}\")\n","\n","if len(TRY_CONFIGS) > 8:\n","    print(f\"  ... total {len(TRY_CONFIGS)} configs\")\n","\n","# ------------------------------------------------------------\n","# 6) vLLM run with retry\n","# ------------------------------------------------------------\n","# ↑ ここからが推論本体です。\n","\n","from vllm import LLM, SamplingParams\n","def run_with_config(cfg):\n","\n","    sampling = SamplingParams(\n","        temperature=TEMPERATURE,\n","        max_tokens=cfg[\"max_tokens\"],\n","    )\n","\n","    llm = LLM(\n","        model=model_path,\n","        max_model_len=cfg[\"max_model_len\"],\n","        gpu_memory_utilization=cfg[\"gpu_mem\"],\n","        enforce_eager=True,\n","        tensor_parallel_size=1,\n","         disable_log_stats=True,\n","    )\n","\n","    outs = llm.generate(prompts, sampling)\n","\n","    submission = []\n","    # ↑ 提出形式 [{\"task_id\": ..., \"generation\": ...}, ...] を作ります。\n","\n","    for tid, out in zip(task_ids, outs):\n","        gen = out.outputs[0].text if out.outputs else \"\"\n","        submission.append({\"task_id\": tid, \"generation\": gen})\n","    return submission\n","    # ↑ 150問ぶんの提出配列を返します。\n","\n","last_err = None\n","submission = None\n","# ↑ 成功した場合に提出データ（150件）を入れる変数。成功まではNone。\n","\n","for idx, cfg in enumerate(TRY_CONFIGS, 1):\n","    print(f\"[INFO] Attempt {idx}/{len(TRY_CONFIGS)}: max_model_len={cfg['max_model_len']} max_tokens={cfg['max_tokens']} gpu_mem={cfg['gpu_mem']}\")\n","    try:\n","        submission = run_with_config(cfg)\n","        print(\"[INFO] ✅ Generation succeeded with this config.\")\n","        # ↑ 成功ログ\n","        break\n","    except RuntimeError as e:\n","        last_err = e\n","        msg = str(e)\n","        print(\"[WARN] Failed:\", msg[:200].replace(\"\\n\", \" \"))\n","\n","# try next config\n","if submission is None:\n","    raise RuntimeError(f\"All configs failed. Last error: {last_err}\")\n","\n","\n","# Final guards\n","# ↑ 最後に「提出物としての整合性チェック」をします。\n","\n","if len(submission) != 150:\n","    # ↑ 150件生成できているかチェック\n","    raise RuntimeError(f\"Submission count mismatch: {len(submission)}\")\n","\n","if len({x['task_id'] for x in submission}) != 150:\n","    # ↑ task_id の重複がないかチェック\n","    raise RuntimeError(\"Duplicate task_id in submission\")\n","\n","Path(OUTPUT_PATH).write_text(json.dumps(submission, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n","# ↑ submission（Pythonオブジェクト）をJSON文字列にしてファイルへ保存します。\n","\n","print(\"[OK] wrote:\", OUTPUT_PATH, \"items=150\")\n"],"metadata":{"id":"rCbWE8H6qDeS"},"execution_count":null,"outputs":[]}]}