{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15498cd033ba4b98af1f16dbd8ee56d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f124e3dba23e42c0b3e49c2d3e208d8c",
              "IPY_MODEL_0fa8089d4ba2488482a287fd656974c0",
              "IPY_MODEL_a925634ec28d408081be4e671a058ba5",
              "IPY_MODEL_ebaf19dcdd514a48af8b999a9248b29a",
              "IPY_MODEL_8d0c8574b9bf49508173d5b0ce992cf2"
            ],
            "layout": "IPY_MODEL_40bf42e0365c4167be00a42b80c5a5e3"
          }
        },
        "f124e3dba23e42c0b3e49c2d3e208d8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c8580d5357441f696463aaef69a8006",
            "placeholder": "​",
            "style": "IPY_MODEL_cb889dc01eba4a05aaf8cf870470cfe1",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "0fa8089d4ba2488482a287fd656974c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_eeb5fb3b40194be3b9d7e26786b5ee8a",
            "placeholder": "​",
            "style": "IPY_MODEL_9636689697e148859c0752fd721c1b40",
            "value": ""
          }
        },
        "a925634ec28d408081be4e671a058ba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_8fb02b0923bf4e12a373ff3be2e1d23e",
            "style": "IPY_MODEL_c6be6e0b7b61409c9080161b884c1d6e",
            "value": true
          }
        },
        "ebaf19dcdd514a48af8b999a9248b29a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_cd495d4e497147048ede55b449fd9579",
            "style": "IPY_MODEL_2c28ad5178844f8f84c052a5dcf19723",
            "tooltip": ""
          }
        },
        "8d0c8574b9bf49508173d5b0ce992cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50b511e663c549bfac9c225c6c96bda6",
            "placeholder": "​",
            "style": "IPY_MODEL_d819b11f87ca43b187b9e54d8e567818",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "40bf42e0365c4167be00a42b80c5a5e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "3c8580d5357441f696463aaef69a8006": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb889dc01eba4a05aaf8cf870470cfe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eeb5fb3b40194be3b9d7e26786b5ee8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9636689697e148859c0752fd721c1b40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fb02b0923bf4e12a373ff3be2e1d23e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6be6e0b7b61409c9080161b884c1d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd495d4e497147048ede55b449fd9579": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c28ad5178844f8f84c052a5dcf19723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "50b511e663c549bfac9c225c6c96bda6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d819b11f87ca43b187b9e54d8e567818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 構造化データセットSFT（Unsloth / Colab T4）標準学習コード：実行ガイド\n",
        "\n",
        "本ノートブックは、**構造化出力を測るベンチマークスコア向上**を目的として、  \n",
        "小型LLM（Qwen3-4B Instruct-2507）に対して **SFT（Supervised Fine-Tuning）** を行う標準コードです。\n",
        "\n",
        "学習は **Unsloth + QLoRA（4bit）** を利用し、**Colab 無料版（T4）**で動作するようにメモリ最適化されています。\n"
      ],
      "metadata": {
        "id": "M6vtQKTcQ8Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BQ2J_Hd6nHp5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U59UcDJnIOO",
        "outputId": "489b0453-e4cd-40c6-90c1-681aff12d0a3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hNvLHM7VnE7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. このコードが行うこと（概要）\n"
      ],
      "metadata": {
        "id": "v0xXIVtoRnQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "このコードは大きく3段階で構成されています。\n",
        "\n",
        "1. **環境固定（依存パッケージのバージョン固定）**  \n",
        "   Colabの環境変化による不具合を避けるため、numpy/transformers/trl/unsloth等を特定バージョンで揃えます。\n",
        "\n",
        "2. **SFT（教師あり微調整）の実行**  \n",
        "   Hugging Face Hub 上の学習データセットを読み込み、ベースモデルに LoRA アダプタを差し込み、学習します。  \n",
        "   学習の損失（loss）は **assistant 出力部分だけ**にかかる設計です（structured output を学習させやすい）。\n",
        "\n",
        "3. **LoRAアダプタのHugging Faceへのアップロード**  \n",
        "   学習で得られた LoRA の重み（adapter）を HF Hub に保存できます。  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0bWkeEfIRZKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 2. 実行手順（最短手順）\n"
      ],
      "metadata": {
        "id": "56C03Zt2Rr3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 0: Colab の準備\n",
        "- ランタイムの種類を **GPU** に変更し、GPU が **T4** になっていることを確認してください。\n",
        "- 過去の実行で環境が壊れている場合は **Runtime > Factory reset** を推奨します。\n",
        "\n",
        "### Step 1: 依存関係インストール\n",
        "- 先頭の `pip uninstall` → `pip install` を上から順に実行します。\n",
        "- 実行後、バージョン表示が想定通りであることを確認します（`unsloth import OK` が出ること）。\n",
        "\n",
        "### Step 2: Hugging Face へログイン\n",
        "- `login()` を実行するとトークン入力が求められます。\n",
        "- 入力するトークンは、**WRITE権限**のものを使用してください。\n",
        "- ※学習データセットが公開ならログイン無しでも読める場合がありますが、標準手順としてログインします。\n",
        "\n",
        "### Step 3: 学習の実行\n",
        "- `main()` が呼ばれ、学習が開始します。\n",
        "- 学習中に `[LabelStats:train]` が表示されます。これは「loss対象トークンが極端にゼロになっていないか」の健康診断です。\n",
        "\n",
        "### Step 4: 学習成果物の確認、LoRAアダプタのhuggingfaceへのアップロード\n",
        "- 学習後、`OUT_LORA_DIR` に以下が保存されます（最低限）：\n",
        "  - `adapter_config.json`\n",
        "  - `adapter_model.safetensors`（または `adapter_model.bin`）\n",
        "  - tokenizer 関連ファイル\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fsijt4sARZM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 出力（何が生成されるか）\n",
        "\n"
      ],
      "metadata": {
        "id": "awaxrALwRxpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `OUT_LORA_DIR`（例：`/content/lora_structeval_t_qwen3_4b`）に、\n",
        "  **LoRAアダプタ（差分重み）**が保存されます。\n",
        "- このアダプタをベースモデルに適用して推論することで、StructEval-T のスコア改善を狙います。\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "PLhfZsZpRZPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 4. 学習データセットの説明\n"
      ],
      "metadata": {
        "id": "fp2GXhJyQ1NZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 4.1 データセット概要\n",
        "本コードで使用するデータセットは以下です：\n",
        "\n",
        "- HF Dataset: `u-10bei/structured_data_with_cot_dataset_512_v2`  \n",
        "  https://huggingface.co/datasets/u-10bei/structured_data_with_cot_dataset_512_v2\n",
        "\n",
        "このデータセットは、**構造化出力（CSV / JSON / XML / TOML / YAML）**を中心とした、\n",
        "形式変換・抽出タスク向けのSFTデータです。\n",
        "\n",
        "### 4.2 収録件数・split\n",
        "- Subset: `default`\n",
        "- Split: `train`\n",
        "- 行数：**約 3.65k rows**\n",
        "\n",
        "### 4.3 カラム（列）構造\n",
        "Viewer上で確認できる代表的なカラムは以下です。\n",
        "\n",
        "- `id`（文字列）\n",
        "- `category`（カテゴリ：複数値）\n",
        "- `subcategory`（サブカテゴリ：複数値）\n",
        "- `task`（タスク種別：複数値）\n",
        "- `seed`（生成や由来を示す識別子）\n",
        "- `messages`（**OpenAI messages形式のlist**）\n",
        "\n",
        "特に重要なのが `messages` で、各サンプルは以下のような形です：\n",
        "\n",
        "```json\n",
        "[\n",
        "  {\"role\": \"user\", \"content\": \"...指示と入力...\"},\n",
        "  {\"role\": \"assistant\", \"content\": \"...期待される出力...\"}\n",
        "]\n"
      ],
      "metadata": {
        "id": "tfLExuPZRZZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 実行コード"
      ],
      "metadata": {
        "id": "iesWWPf3RZdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1:依存関係インストール"
      ],
      "metadata": {
        "id": "m9UNQU8fPoGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 0) 依存関係の固定（Colabの“環境ブレ”対策）\n",
        "# ============================================================\n",
        "# Colab（無料版）は、ある日突然プリインストール版が変わり、\n",
        "# それまで動いていた学習コードが壊れることが頻繁にあります。\n",
        "# そのため、このセルでは「一度全部消す → 互換が確認できたバージョンを入れ直す」\n",
        "# という“強制的な再現性確保”をしています。\n",
        "#\n",
        "# ※Errorが出力されることがありますが、「使用しないライブラリ」に関するエラーであれば、関係なく動作します。\n",
        "\n",
        "!pip -q uninstall -y numpy pandas datasets trl transformers accelerate peft unsloth unsloth-zoo bitsandbytes xformers\n",
        "!pip -q install \"numpy==2.0.2\" \"pandas==2.2.2\"\n",
        "!pip -q install \\\n",
        "  \"datasets==4.3.0\" \\\n",
        "  \"trl==0.24.0\" \\\n",
        "  \"transformers==4.56.2\" \\\n",
        "  \"accelerate==1.1.0\" \\\n",
        "  \"peft==0.13.2\" \\\n",
        "  \"bitsandbytes==0.45.0\"\n",
        "# unsloth / zoo を同系列で揃える（zoo側の要求に合わせる）\n",
        "# Unsloth本体と unsloth-zoo は“セット運用”が基本です。片方だけ上げると壊れがちです。\n",
        "!pip -q install \"unsloth-zoo==2025.12.7\" \"unsloth==2025.12.7\"\n",
        "\n"
      ],
      "metadata": {
        "id": "Bf7hi5wBSofx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1821566b-9b8d-42f0-cb6c-fe8080098c5a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtune 0.6.1 requires datasets, which is not installed.\n",
            "sentence-transformers 5.2.2 requires transformers<6.0.0,>=4.41.0, which is not installed.\n",
            "fastai 2.8.6 requires torch<2.10,>=1.10, but you have torch 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Cannot install accelerate==1.1.0 and trl==0.24.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ============================================================\n",
        "# 0.1) バージョン確認（“動くはず”の状態かを目視で確認）\n",
        "# ============================================================\n",
        "# ここで想定バージョンとズレている場合、\n",
        "# 後工程で原因不明のエラーが出る確率が一気に上がります。\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "import datasets, trl, transformers, torch\n",
        "\n",
        "print(\"numpy\", np.__version__)\n",
        "print(\"pandas\", pd.__version__)\n",
        "print(\"datasets\", datasets.__version__)\n",
        "print(\"trl\", trl.__version__)\n",
        "print(\"transformers\", transformers.__version__)\n",
        "print(\"torch\", torch.__version__)\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "print(\"unsloth import OK\")\n",
        "\n",
        "# 期待値：\n",
        "# numpy 2.0.2\n",
        "# pandas 2.2.2\n",
        "# datasets 4.3.0（または <4.4.0 で 4.0.* / 4.1.0 以外）\n",
        "# trl 0.24.0（または 0.18.2〜0.24.0 で 0.19.0以外）\n",
        "# unsloth import OK\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 0) Install (single cell)\n",
        "# -----------------------------\n",
        "# NOTE:\n",
        "# - Colabは初期状態が頻繁に変わるため、ピン留めで安定化します。\n",
        "#   もし依存関係が壊れている環境であれば、Runtime > Factory reset を推奨。\n",
        "\n",
        "# このセルを実行して、上の「期待値」にもしなっていない場合は、下記のコメントアウトを外して実行してみてください。\n",
        "# !pip -q install -U \\\n",
        "#   \"numpy==2.0.2\" \\\n",
        "#   \"pandas==2.2.2\" \\\n",
        "#   \"datasets==4.3.0\" \\\n",
        "#   \"trl==0.24.0\" \\\n",
        "#   \"transformers==4.57.3\" \\又は、4.56.2\n",
        "#   \"accelerate==1.1.0\" \\\n",
        "#   \"peft==0.13.2\" \\\n",
        "#   \"bitsandbytes==0.45.0\" \\\n",
        "#   \"unsloth-zoo==2025.12.7\" \\\n",
        "#   \"unsloth==2025.12.7\" \\\n",
        "#   \"huggingface_hub\"\n"
      ],
      "metadata": {
        "id": "JHkB4LAjS1Z2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaea99bf-f0a2-4be4-aa05-fa3b84d2dca5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy 2.0.2\n",
            "pandas 2.2.2\n",
            "datasets 4.3.0\n",
            "trl 0.24.0\n",
            "transformers 4.57.3\n",
            "torch 2.10.0+cu128\n",
            "unsloth import OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: HuggingFace ログイン\n",
        "\n",
        "Hugging Faceに自分のモデルやデータセットを保存したり、設定を変更したりするには、書き込み用の「トークン」が必要です。\n",
        "トークンは、以下の手順で取得できます。\n",
        "\n",
        "ステップ1：設定画面を開く\n",
        "- Hugging Face にログインします。(https://huggingface.co/)\n",
        "- 画面右上の自分のアイコンをクリックします。\n",
        "- メニューの中から 「Settings」（設定）を選択します。\n",
        "\n",
        "ステップ2：アクセストークンのページへ\n",
        "- 左側のサイドメニューにある 「Access Tokens」 をクリックします。\n",
        "\n",
        "ステップ3：新しいトークンを作成する\n",
        "- 画面中央にある 「+ Create new token」 ボタンをクリックします。\n",
        "- 設定ウィンドウが開くので、以下の2項目を入力・選択します。\n",
        "- Token Name: 自分が分かりやすい名前を付けます（例：my-upload-token など）。\n",
        "- Token type: ここが一番重要です！今回は、学習後のモデル（アダプタ）をアップロードするため、必ず 「Write」 を選択してください。\n",
        "- 下にある 「Create token」 ボタンを押して完了です。\n",
        "\n",
        "ステップ4：トークンをコピーして保存する\n",
        "- 作成されたトークンの横にある コピーアイコン（紙が重なったマーク） をクリックして、トークンをコピーします。\n",
        "\n",
        "- コピーした文字列は、メモ帳などに貼り付けて大切に保管してください。\n",
        "\n",
        "⚠️ 大切な注意点\n",
        "トークンは「パスワード」と同じです： このトークンが他人に知られると、あなたのリポジトリを勝手に書き換えられてしまう恐れがあります。GitHubなどにそのまま貼り付けて公開しないよう、十分注意してください。"
      ],
      "metadata": {
        "id": "mi78wnsDP6Yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# 1) HF login (once)\n",
        "# -----------------------------\n",
        "# Hugging Face（HF）はモデルやデータセットをホスティングするサービスです。\n",
        "# このコードでは「HF Hub上のデータセットを読む」「学習したLoRAをHFにアップする」ためにログインします。\n",
        "#\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import numpy as np, pandas as pd\n",
        "import datasets, trl, transformers, torch\n",
        "\n",
        "from huggingface_hub import login, HfApi\n",
        "login()  # Colab will prompt\n",
        "api = HfApi()"
      ],
      "metadata": {
        "id": "SrjUvUoPP6i9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "15498cd033ba4b98af1f16dbd8ee56d2",
            "f124e3dba23e42c0b3e49c2d3e208d8c",
            "0fa8089d4ba2488482a287fd656974c0",
            "a925634ec28d408081be4e671a058ba5",
            "ebaf19dcdd514a48af8b999a9248b29a",
            "8d0c8574b9bf49508173d5b0ce992cf2",
            "40bf42e0365c4167be00a42b80c5a5e3",
            "3c8580d5357441f696463aaef69a8006",
            "cb889dc01eba4a05aaf8cf870470cfe1",
            "eeb5fb3b40194be3b9d7e26786b5ee8a",
            "9636689697e148859c0752fd721c1b40",
            "8fb02b0923bf4e12a373ff3be2e1d23e",
            "c6be6e0b7b61409c9080161b884c1d6e",
            "cd495d4e497147048ede55b449fd9579",
            "2c28ad5178844f8f84c052a5dcf19723",
            "50b511e663c549bfac9c225c6c96bda6",
            "d819b11f87ca43b187b9e54d8e567818"
          ]
        },
        "outputId": "dc6e287b-22ec-453d-be50-ba9cdafd1181"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15498cd033ba4b98af1f16dbd8ee56d2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step3:学習の実行"
      ],
      "metadata": {
        "id": "3YyMQDqNRI2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# 2) Training code\n",
        "# ============================================================\n",
        "# ここからがSFT本体です。\n",
        "# 大まかな流れ：\n",
        "#  1) 設定値（モデル名、データセット、LoRA設定、学習率など）を読み込む\n",
        "#  2) データセットをHFから取得し、必要な形（messages形式）を満たすものだけ残す\n",
        "#  3) tokenizerで「学習に使うテキスト」を作ってキャッシュする（高速化）\n",
        "#  4) ベースモデルを4bitでロードし、LoRAアダプタを差し込む\n",
        "#  5) Trainerで学習を回す\n",
        "#  6) LoRAアダプタを保存する\n"
      ],
      "metadata": {
        "id": "QXdP-QwRQg3G"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ベースモデル：Qwen3-4B-Instruct-2507\n",
        "- GPU：T4（無料Colab）でも回るように、メモリ節約を強く意識しています。\n",
        "- 学習方式：QLoRA（4bitでベースを読み、LoRAアダプタのみ学習）\n",
        "  - “全部の重み”を学習するのではなく、LoRAアダプタ（軽量差分）だけを学習します。\n",
        "  - そのため、学習後に保存されるのも「アダプタ」中心になります。"
      ],
      "metadata": {
        "id": "QRNFLLFANPS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 使用可能なデータセット\n",
        "今回、運営において9種類の合成データセットを用意しました。\n",
        "\n",
        "- 1-1. https://huggingface.co/datasets/u-10bei/structured_data_with_cot_dataset_512_v2\n",
        "- 1-2. https://huggingface.co/datasets/u-10bei/structured_data_with_cot_dataset_512_v4\n",
        "- 1-3. https://huggingface.co/datasets/u-10bei/structured_data_with_cot_dataset_512_v5\n",
        "- 1-4.\n",
        "https://huggingface.co/datasets/u-10bei/structured_data_with_cot_dataset_512\n",
        "- 1-5.\n",
        "https://huggingface.co/datasets/u-10bei/structured_data_with_cot_dataset_v2\n",
        "- 1-6.\n",
        "https://huggingface.co/datasets/u-10bei/structured_data_with_cot_dataset\n",
        "- 2-1. https://huggingface.co/datasets/daichira/structured-3k-mix-sft\n",
        "- 2-2. https://huggingface.co/datasets/daichira/structured-5k-mix-sft\n",
        "- 2-3. https://huggingface.co/datasets/daichira/structured-hard-sft-4k\n",
        "\n",
        " この標準コードでは1-1を使用していますが、1-2以降を使用してもOKです。\n",
        " - 学習データセットを1-1以外に変更せずとも、後述の環境変数（4.ハイパーパラメータ）を変更することにより、モデル性能が向上する（修了要件を満たす）ことが可能です。\n",
        " - さらなる性能向上のため、これらのデータセットに追加で前処理を行ってから学習を行っても差し支えありません。\n",
        "\n",
        " 注意\n",
        "- このデータを使用するとスコアが上がることを保証するものではありません．\n",
        "- ご自身で組み合わせたり，カスタマイズして使用してみてください．\n",
        "- ただし，詳細資料に記載してあるルールは守ってください．\n"
      ],
      "metadata": {
        "id": "2ok0cPOcRbTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import shutil\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import TrainingArguments, Trainer, TrainerCallback\n"
      ],
      "metadata": {
        "id": "NK6nsyrSQdyF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 環境変数の設定\n",
        "# -----------------------------\n",
        "# 下記の値を書き換えることで、コード本体を編集せずに設定を変更できます。\n",
        "\n",
        "# 1. モデル・データセット関連\n",
        "os.environ[\"SFT_BASE_MODEL\"] = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "os.environ[\"SFT_DATASET_ID\"] = \"u-10bei/structured_data_with_cot_dataset_512_v2\"\n",
        "os.environ[\"SFT_OUT_LORA_DIR\"] = \"/content/lora_structeval_t_qwen3_4b\"\n",
        "\n",
        "# 2. 学習の基本パラメータ\n",
        "os.environ[\"SFT_SEED\"] = \"3407\"\n",
        "os.environ[\"SFT_VAL_RATIO\"] = \"0.05\"\n",
        "os.environ[\"SFT_MAX_SEQ_LEN\"] = \"512\"\n",
        "\n",
        "# 3. LoRA (アダプタ) 設定\n",
        "os.environ[\"SFT_LORA_R\"] = \"128\"\n",
        "os.environ[\"SFT_LORA_ALPHA\"] = \"256\"\n",
        "os.environ[\"SFT_LORA_DROPOUT\"] = \"0\"\n",
        "os.environ[\"SFT_LORA_TARGET_MODULES\"] = \"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n",
        "\n",
        "# 4. ハイパーパラメータ\n",
        "os.environ[\"SFT_EPOCHS\"] = \"1\"\n",
        "os.environ[\"SFT_PER_DEVICE_TRAIN_BS\"] = \"2\"\n",
        "os.environ[\"SFT_PER_DEVICE_EVAL_BS\"] = \"2\"\n",
        "os.environ[\"SFT_GRAD_ACCUM\"] = \"8\"\n",
        "os.environ[\"SFT_LR\"] = \"1e-6\"\n",
        "os.environ[\"SFT_WARMUP_RATIO\"] = \"0.1\"\n",
        "os.environ[\"SFT_WEIGHT_DECAY\"] = \"0.05\"\n",
        "\n",
        "# 5. ステップ・保存設定\n",
        "os.environ[\"SFT_MAX_STEPS\"] = \"-1\" # -1でエポックベース。動作確認時は 10 などに。\n",
        "os.environ[\"SFT_LOGGING_STEPS\"] = \"10\"\n",
        "os.environ[\"SFT_EVAL_STEPS\"] = \"50\"\n",
        "os.environ[\"SFT_SAVE_STEPS\"] = \"100\"\n",
        "os.environ[\"SFT_SAVE_TOTAL_LIMIT\"] = \"2\"\n",
        "\n",
        "# 6. 特殊学習設定 (CoTマスク・アップサンプリング)\n",
        "os.environ[\"SFT_MASK_COT\"] = \"1\" # \"1\" で有効, \"0\" で無効\n",
        "os.environ[\"SFT_OUTPUT_MARKERS\"] = \"Output:,OUTPUT:,Final:,Answer:,Result:,Response:\"\n",
        "os.environ[\"SFT_OUTPUT_LEARN_MODE\"] = \"after_marker\" # \"after_marker\" または \"from_marker\"\n",
        "os.environ[\"SFT_USE_UPSAMPLING\"] = \"0\" # \"1\" で有効, \"0\" で無効  # データ2-1,2-2,2-3 専用\n",
        "os.environ[\"SFT_UPSAMPLE_RULES\"] = '{\"xml_to_yaml\": 2.0}' # 例: '{\"json_to_xml\": 1.8, \"text_to_yaml\": 1.6}' # データ2-1,2-2,2-3 専用\n",
        "\n",
        "print(\"環境変数の設定が完了しました。\")"
      ],
      "metadata": {
        "id": "z7t5SanBUJJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e259646-599e-4cce-a04c-53c3de63c925"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "環境変数の設定が完了しました。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOf9hrSaNl9x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e4d70d3-7668-4a29-924a-1db3c0c099c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading dataset from HF Hub: u-10bei/structured_data_with_cot_dataset_512_v2\n",
            "[INFO] Loading base model: Qwen/Qwen3-4B-Instruct-2507\n",
            "==((====))==  Unsloth 2025.12.7: Fast Qwen3 patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.34. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "[INFO] Checking all-masked samples before filtering...\n",
            "[CHECK] all-masked samples in 197: 9 (4.6%)\n",
            "[INFO] Filtering train/val to remove all-masked samples...\n",
            "[INFO] New sizes: train = 3547 val = 187\n",
            "[INFO] Checking all-masked samples after filtering...\n",
            "[CHECK] all-masked samples in 187: 0 (0.0%)\n",
            "[INFO] Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-661023267.py:509: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer._unsloth___init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 3,547 | Num Epochs = 1 | Total steps = 222\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 264,241,152 of 4,286,709,248 (6.16% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='138' max='222' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [138/222 33:12 < 20:30, 0.07 it/s, Epoch 0.62/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.480900</td>\n",
              "      <td>1.655032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.447900</td>\n",
              "      <td>1.522384</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[LabelStats:train] step=10 valid_ratio=0.4548\n",
            "\n",
            "[LabelStats:train] step=20 valid_ratio=0.4786\n",
            "\n",
            "[LabelStats:train] step=30 valid_ratio=0.4888\n",
            "\n",
            "[LabelStats:train] step=40 valid_ratio=0.5755\n",
            "\n",
            "[LabelStats:train] step=50 valid_ratio=0.4728\n",
            "\n",
            "[LabelStats:train] step=60 valid_ratio=0.6064\n",
            "\n",
            "[LabelStats:train] step=70 valid_ratio=0.5915\n",
            "\n",
            "[LabelStats:train] step=80 valid_ratio=0.5277\n",
            "\n",
            "[LabelStats:train] step=90 valid_ratio=0.5774\n",
            "\n",
            "[LabelStats:train] step=100 valid_ratio=0.4613\n",
            "\n",
            "[LabelStats:train] step=110 valid_ratio=0.5955\n",
            "\n",
            "[LabelStats:train] step=120 valid_ratio=0.5868\n",
            "\n",
            "[LabelStats:train] step=130 valid_ratio=0.6039\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# 2.1) Config (env-overridable)\n",
        "# -----------------------------\n",
        "# “環境変数で上書きできる設定”を用意しています。\n",
        "# つまり、コードを編集しなくても、Colabの環境変数を変えるだけで\n",
        "# ベースモデル名、学習率、エポック数などを変更できる設計です。\n",
        "#\n",
        "# この設計のメリット：\n",
        "# - “標準コード”は同じまま、ハイパーパラメータだけ試せる（再現性が高い）\n",
        "\n",
        "def _getenv(name: str, default: str):\n",
        "    return os.environ.get(name, default)\n",
        "\n",
        "def _getenv_int(name: str, default: int) -> int:\n",
        "    try:\n",
        "        return int(os.environ.get(name, str(default)))\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "def _getenv_float(name: str, default: float) -> float:\n",
        "    try:\n",
        "        return float(os.environ.get(name, str(default)))\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "# 学習の“出発点”となるベースモデル（4B）\n",
        "BASE_MODEL_ID = _getenv(\"SFT_BASE_MODEL\", \"Qwen/Qwen3-4B-Instruct-2507\")\n",
        "\n",
        "# 学習に使うSFTデータセット（HF Hub上に置かれている想定）\n",
        "DATASET_ID    = _getenv(\"SFT_DATASET_ID\", \"u-10bei/structured_data_with_cot_dataset_512_v2\")\n",
        "\n",
        "# 学習後に保存されるLoRAアダプタの出力先（ローカル）\n",
        "OUT_LORA_DIR  = _getenv(\"SFT_OUT_LORA_DIR\", \"/content/lora_structeval_t_qwen3_4b\") # HFアップロードするアダプタ名と合わせる\n",
        "\n",
        "SEED        = _getenv_int(\"SFT_SEED\", 3407)\n",
        "VAL_RATIO   = _getenv_float(\"SFT_VAL_RATIO\", 0.05)\n",
        "\n",
        "# 1サンプルあたり最大何トークンまで見るか（長いほど情報を見られるが、GPUメモリと時間が増える）\n",
        "MAX_SEQ_LEN = _getenv_int(\"SFT_MAX_SEQ_LEN\", 512)\n",
        "\n",
        "# LoRA Config（＝“どれくらいの表現力を持つ差分を学習するか”）\n",
        "LORA_R       = _getenv_int(\"SFT_LORA_R\", 96)\n",
        "LORA_ALPHA   = _getenv_int(\"SFT_LORA_ALPHA\", 192)\n",
        "LORA_DROPOUT = _getenv_float(\"SFT_LORA_DROPOUT\", 0)\n",
        "LORA_TARGET_MODULES = (\n",
        "    _getenv(\"SFT_LORA_TARGET_MODULES\", \"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\").split(\",\")\n",
        ")\n",
        "\n",
        "# Train hyperparams（学習の基本設定）\n",
        "NUM_TRAIN_EPOCHS            = _getenv_int(\"SFT_EPOCHS\", 1)\n",
        "PER_DEVICE_TRAIN_BATCH_SIZE = _getenv_int(\"SFT_PER_DEVICE_TRAIN_BS\", 2)\n",
        "PER_DEVICE_EVAL_BATCH_SIZE  = _getenv_int(\"SFT_PER_DEVICE_EVAL_BS\", 2)\n",
        "\n",
        "# 勾配累積：GPUに一度に載せられるバッチが小さい時に、複数ステップ分を貯めて“大きいバッチ相当”にする\n",
        "GRAD_ACCUM                  = _getenv_int(\"SFT_GRAD_ACCUM\", 8)\n",
        "\n",
        "LR                          = _getenv_float(\"SFT_LR\", 1e-6)\n",
        "WARMUP_RATIO                = _getenv_float(\"SFT_WARMUP_RATIO\", 0.1)\n",
        "\n",
        "# Debug / quick check\n",
        "# MAX_STEPSを小さくすると“動作確認だけ”の短時間学習ができます（本番は -1 のまま）\n",
        "MAX_STEPS        = _getenv_int(\"SFT_MAX_STEPS\", -1)\n",
        "LOGGING_STEPS    = _getenv_int(\"SFT_LOGGING_STEPS\", 10)\n",
        "EVAL_STEPS       = _getenv_int(\"SFT_EVAL_STEPS\", 50)\n",
        "SAVE_STEPS       = _getenv_int(\"SFT_SAVE_STEPS\", 100)\n",
        "SAVE_TOTAL_LIMIT = _getenv_int(\"SFT_SAVE_TOTAL_LIMIT\", 2)\n",
        "WEIGHT_DECAY     = _getenv_float(\"SFT_WEIGHT_DECAY\", 0.05)\n",
        "\n",
        "# Optional: upsampling rules\n",
        "# 特定のサブカテゴリ（例：難しいタスク）を“多めに学習させる”ための仕組み。\n",
        "# 標準ではOFFになっています。\n",
        "UPSAMPLE_ENABLE     = _getenv(\"SFT_USE_UPSAMPLING\", \"0\") in (\"1\",\"true\",\"True\")\n",
        "UPSAMPLE_RULES_JSON = _getenv(\"SFT_UPSAMPLE_RULES\", \"\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2.2) Seed & Utils\n",
        "# -----------------------------\n",
        "# 乱数（シャッフルやサンプリング）を固定して、再現性を担保します。\n",
        "# seedが同じなら、原則として同じ分割・同じ抽出になりやすいです。\n",
        "\n",
        "def seed_everything(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(SEED)\n",
        "\n",
        "def ensure_openai_messages(ds: Dataset, msg_col: str = \"messages\") -> None:\n",
        "    # データが「messages: [{role, content}, ...]」形式かをチェックします。\n",
        "    # これは ChatGPT形式（OpenAIのChat Completions形式に似た）で、\n",
        "    # tokenizer.apply_chat_template で安全に文字列化するために必要です。\n",
        "    row0 = ds[0]\n",
        "    ex = row0.get(msg_col, None)\n",
        "    if not isinstance(ex, list):\n",
        "        raise ValueError(f\"Dataset must have list-style 'messages'. Got {type(ex)}\")\n",
        "\n",
        "def has_any_nonempty_assistant_turn(msgs: List[Dict[str, Any]]) -> bool:\n",
        "    # “assistantの発話が空じゃない”ものが1回でも含まれるか？\n",
        "    # SFTでは「正解例（assistantの出力）」がないと学習できないため。\n",
        "    return any(m.get(\"role\") == \"assistant\" and str(m.get(\"content\", \"\")).strip() != \"\" for m in msgs)\n",
        "\n",
        "def ends_with_nonempty_assistant(ex: Dict[str, Any]) -> bool:\n",
        "    # 最後のターンが assistant の回答になっているサンプルだけを使います。\n",
        "    # こうしておくと「最後のassistantだけ学習する（assistant-only loss）」設計と相性が良いです。\n",
        "    msgs = ex.get(\"messages\", [])\n",
        "    if not msgs or msgs[-1].get(\"role\") != \"assistant\":\n",
        "        return False\n",
        "    c = msgs[-1].get(\"content\", \"\")\n",
        "    return isinstance(c, str) and c.strip() != \"\"\n",
        "\n",
        "def shuffle_split(ds: Dataset, val_ratio: float, seed: int) -> Tuple[Dataset, Dataset]:\n",
        "    # データをシャッフルして train/val に分割します。\n",
        "    # val（検証）を持つことで「学習が進むほど性能が上がっているか／過学習していないか」を見られます。\n",
        "    ds_shuf = ds.shuffle(seed=seed)\n",
        "    n = len(ds_shuf)\n",
        "    n_val = max(1, int(round(n * val_ratio)))\n",
        "    return ds_shuf.select(range(n_val, n)), ds_shuf.select(range(n_val))\n",
        "\n",
        "def make_text_cache_builder(tokenizer):\n",
        "    # messages形式 → 実際にモデルに入力する“1本のテキスト”へ変換する関数を作ります。さらに「トークン長（truncationなし）」もキャッシュします。\n",
        "    #\n",
        "    # full_text  : ユーザー＋アシスタント（正解）まで含んだ全文\n",
        "    # prefix_text: “最後のassistantの直前まで”の文（＝ここからassistantを生成させたい）\n",
        "    #\n",
        "    # この2つを持つことで、後のcollatorで「assistant部分だけをloss対象にする境界」を計算できます。\n",
        "\n",
        "    def _build(batch):\n",
        "        full_out = []\n",
        "        prefix_out = []\n",
        "        full_len_out = []\n",
        "        prefix_len_out = []\n",
        "\n",
        "        for msgs in batch[\"messages\"]:\n",
        "            full = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)\n",
        "            prefix = tokenizer.apply_chat_template(msgs[:-1], tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "            full_out.append(full)\n",
        "            prefix_out.append(prefix)\n",
        "\n",
        "            # 重要：ここで truncation=False で token 長だけ計算してキャッシュする\n",
        "            # add_special_tokens=False はあなたの現行設計に合わせる（テンプレ側で必要トークンが入る想定）\n",
        "            full_ids = tokenizer(full, add_special_tokens=False, truncation=False)[\"input_ids\"]\n",
        "            prefix_ids = tokenizer(prefix, add_special_tokens=False, truncation=False)[\"input_ids\"]\n",
        "\n",
        "            full_len_out.append(len(full_ids))\n",
        "            prefix_len_out.append(len(prefix_ids))\n",
        "\n",
        "        return {\n",
        "            \"full_text\": full_out,\n",
        "            \"prefix_text\": prefix_out,\n",
        "            \"full_input_ids_len\": full_len_out,\n",
        "            \"prefix_input_ids_len\": prefix_len_out,\n",
        "        }\n",
        "\n",
        "    return _build\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2.3) Collator (assistant-only loss)\n",
        "# -----------------------------\n",
        "# collatorは「生のサンプル群 → 学習に必要なテンソル(input_ids/labels等)」に変換する部品です。\n",
        "#\n",
        "# ここがこの学習コードの“設計思想”の核心：\n",
        "# - 入力（user/system）も含めてモデルには読ませる\n",
        "# - ただし loss（誤差）を計算するのは assistant の出力部分だけ\n",
        "#\n",
        "# これにより：\n",
        "# - 「プロンプトを丸暗記させる」方向に学習が引っ張られにくい\n",
        "# - “回答の形式”や“出力の正確さ”に学習の力点を置きます。\n",
        "\n",
        "# 使用データセットによる仕様の違い\n",
        "# データセット1：Output: が 100% なので CoT マスクが常に動き、Output本体だけ学習\n",
        "# データセット2：Output: 系ラベルが存在しないため、CoTマスクは発動せず、“出力本体”を学習\n",
        "\n",
        "# --- CoT mask settings (env overridable) ---\n",
        "MASK_COT = _getenv(\"SFT_MASK_COT\", \"1\") in (\"1\",\"true\",\"True\")\n",
        "OUTPUT_MARKERS = [s.strip() for s in _getenv(\n",
        "    \"SFT_OUTPUT_MARKERS\",\n",
        "    \"Output:,OUTPUT:,Final:,Answer:,Result:,Response:\"\n",
        ").split(\",\") if s.strip()]\n",
        "OUTPUT_LEARN_MODE = _getenv(\"SFT_OUTPUT_LEARN_MODE\", \"after_marker\")  # after_marker / from_marker\n",
        "\n",
        "@dataclass\n",
        "class AssistantOnlyCollatorCached:\n",
        "    tokenizer: Any\n",
        "    max_length: int = MAX_SEQ_LEN\n",
        "\n",
        "    def _find_subsequence(self, seq: List[int], sub: List[int]) -> int:\n",
        "        if not sub or len(sub) > len(seq):\n",
        "            return -1\n",
        "        for i in range(0, len(seq) - len(sub) + 1):\n",
        "            if seq[i:i+len(sub)] == sub:\n",
        "                return i\n",
        "        return -1\n",
        "\n",
        "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        tok = self.tokenizer\n",
        "        full_texts   = [ex[\"full_text\"] for ex in batch]\n",
        "        prefix_texts = [ex[\"prefix_text\"] for ex in batch]\n",
        "\n",
        "        old_trunc = getattr(tok, \"truncation_side\", \"right\")\n",
        "        old_pad   = getattr(tok, \"padding_side\", \"right\")\n",
        "        tok.truncation_side = \"left\"\n",
        "        tok.padding_side    = \"right\"\n",
        "\n",
        "        try:\n",
        "            full_enc_tr = tok(\n",
        "                full_texts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=self.max_length,\n",
        "                add_special_tokens=False,\n",
        "            )\n",
        "            input_ids = full_enc_tr[\"input_ids\"]\n",
        "            attention_mask = full_enc_tr[\"attention_mask\"]\n",
        "            labels = torch.full_like(input_ids, fill_value=-100)\n",
        "\n",
        "            full_ids_nt   = tok(full_texts,   return_tensors=None, padding=False, truncation=False, add_special_tokens=False)[\"input_ids\"]\n",
        "            prefix_ids_nt = tok(prefix_texts, return_tensors=None, padding=False, truncation=False, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "            marker_token_seqs = []\n",
        "            if MASK_COT and OUTPUT_MARKERS:\n",
        "                for m in OUTPUT_MARKERS:\n",
        "                    mid = tok(m, add_special_tokens=False, truncation=False)[\"input_ids\"]\n",
        "                    if not mid:\n",
        "                        continue\n",
        "                    mid_nl = tok(m + \"\\n\", add_special_tokens=False, truncation=False)[\"input_ids\"]\n",
        "                    mid_crlf = tok(m + \"\\r\\n\", add_special_tokens=False, truncation=False)[\"input_ids\"]\n",
        "                    marker_token_seqs.append((mid, mid_nl, mid_crlf))\n",
        "\n",
        "            for i in range(input_ids.size(0)):\n",
        "                trunc_left = max(0, len(full_ids_nt[i]) - self.max_length)\n",
        "                boundary = len(prefix_ids_nt[i]) - trunc_left\n",
        "                full_len_tr = int(attention_mask[i].sum().item())\n",
        "\n",
        "                # assistant開始が見えていない => 学習対象外（元コード方針を維持）\n",
        "                if boundary <= 0 or boundary >= full_len_tr:\n",
        "                    continue\n",
        "\n",
        "                span_start = boundary\n",
        "                span_end   = full_len_tr\n",
        "\n",
        "                # デフォルト：assistant全体を学習（データセット2はここに落ちる）\n",
        "                learn_start = span_start\n",
        "\n",
        "                # CoTマスク：Output marker が見つかったときだけ学習開始点を進める（データセット1で発動）\n",
        "                if MASK_COT and marker_token_seqs:\n",
        "                    visible_ids = input_ids[i, :full_len_tr].tolist()\n",
        "                    assistant_ids = visible_ids[span_start:span_end]\n",
        "\n",
        "                    best_out = None  # (out_pos, after_pos)\n",
        "                    for mid, mid_nl, mid_crlf in marker_token_seqs:\n",
        "                        # 改行付き優先\n",
        "                        p = self._find_subsequence(assistant_ids, mid_nl)\n",
        "                        if p != -1:\n",
        "                            out_pos = span_start + p\n",
        "                            after_pos = out_pos + len(mid_nl)\n",
        "                        else:\n",
        "                            p = self._find_subsequence(assistant_ids, mid_crlf)\n",
        "                            if p != -1:\n",
        "                                out_pos = span_start + p\n",
        "                                after_pos = out_pos + len(mid_crlf)\n",
        "                            else:\n",
        "                                p = self._find_subsequence(assistant_ids, mid)\n",
        "                                if p == -1:\n",
        "                                    continue\n",
        "                                out_pos = span_start + p\n",
        "                                after_pos = out_pos + len(mid)\n",
        "\n",
        "                        if (best_out is None) or (out_pos < best_out[0]):\n",
        "                            best_out = (out_pos, after_pos)\n",
        "\n",
        "                    if best_out is not None:\n",
        "                        out_pos, after_pos = best_out\n",
        "                        if OUTPUT_LEARN_MODE == \"from_marker\":\n",
        "                            learn_start = out_pos\n",
        "                        else:\n",
        "                            learn_start = after_pos\n",
        "                        learn_start = max(span_start, min(learn_start, span_end))\n",
        "\n",
        "                if learn_start < span_end:\n",
        "                    labels[i, learn_start:span_end] = input_ids[i, learn_start:span_end]\n",
        "\n",
        "            labels[attention_mask == 0] = -100\n",
        "            return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "        finally:\n",
        "            tok.truncation_side = old_trunc\n",
        "            tok.padding_side    = old_pad\n",
        "\n",
        "import random, torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def filter_has_supervision(ds, collator):\n",
        "    keep = []\n",
        "    for i in range(len(ds)):\n",
        "        out = collator([ds[i]])\n",
        "        if (out[\"labels\"][0] != -100).sum().item() > 0:\n",
        "            keep.append(i)\n",
        "    return ds.select(keep)\n",
        "\n",
        "\n",
        "def count_all_masked(ds, collator, n=200, seed=3407):\n",
        "    rng = random.Random(seed)\n",
        "    n = min(n, len(ds))\n",
        "    idxs = [rng.randrange(0, len(ds)) for _ in range(n)]\n",
        "    all_masked = 0\n",
        "    for i in idxs:\n",
        "        out = collator([ds[i]])\n",
        "        labels = out[\"labels\"][0]\n",
        "        if (labels != -100).sum().item() == 0:\n",
        "            all_masked += 1\n",
        "    print(f\"[CHECK] all-masked samples in {n}: {all_masked} ({all_masked/max(1,n):.1%})\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2.4) Optional upsampling\n",
        "# -----------------------------\n",
        "# upsamplingは「特定の種類のデータを多めに学習させる」テクニックです。\n",
        "# 例：\n",
        "# - JSONは得意だがYAMLは苦手 → YAML関連サンプルを2倍にする\n",
        "# - 特定のsubcategoryが点数に効く → そこを厚くする\n",
        "# ただし、やりすぎると他が弱くなることもあります（トレードオフ）。\n",
        "# 学習データセットの品質が悪い等の原因で、却って性能が低下することもあります。\n",
        "# その場合、学習データセットを観察し、追加の前処理が有効であることも多いです。\n",
        "\n",
        "def apply_upsampling(train_ds: Dataset) -> Dataset:\n",
        "    if not UPSAMPLE_ENABLE or not UPSAMPLE_RULES_JSON:\n",
        "        return train_ds\n",
        "    try:\n",
        "        rules = json.loads(UPSAMPLE_RULES_JSON)\n",
        "        if not isinstance(rules, dict) or not rules:\n",
        "            return train_ds\n",
        "    except Exception:\n",
        "        return train_ds\n",
        "\n",
        "    packs = train_ds[\"subcategory\"] if \"subcategory\" in train_ds.column_names else [None]*len(train_ds)\n",
        "    pack_field = train_ds[\"pack\"] if \"pack\" in train_ds.column_names else [None]*len(train_ds)\n",
        "\n",
        "    w = []\n",
        "    for sub, pk in zip(packs, pack_field):\n",
        "        weight = 1.0\n",
        "        ssub = str(sub or \"\")\n",
        "        spk  = str(pk or \"\")\n",
        "        for pat, mult in rules.items():\n",
        "            try:\n",
        "                m = float(mult)\n",
        "            except Exception:\n",
        "                m = 1.0\n",
        "            if pat.startswith(\"pack:\"):\n",
        "                if spk == pat.split(\":\",1)[1]:\n",
        "                    weight *= max(0.0, m)\n",
        "            else:\n",
        "                if pat in ssub:\n",
        "                    weight *= max(0.0, m)\n",
        "        w.append(weight)\n",
        "\n",
        "    w = np.asarray(w, dtype=np.float64)\n",
        "    if (w <= 0).all() or w.sum() == 0:\n",
        "        return train_ds\n",
        "\n",
        "    p = w / w.sum()\n",
        "    n = len(train_ds)\n",
        "    idx = np.random.choice(np.arange(n), size=n, replace=True, p=p)\n",
        "    print(\"[UPSAMPLE] rules:\", rules)\n",
        "    return train_ds.select(idx.tolist())\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2.5) Callback (monitor)\n",
        "# -----------------------------\n",
        "# 学習中のデバッグ用コールバックです。\n",
        "# ここでは「labelsのうち、実際にloss対象になっているトークン割合」を時々表示します。\n",
        "#\n",
        "# 意味：\n",
        "# - valid_ratio が極端に小さい → “学習していない”のと同じ（ラベルがほぼ -100）\n",
        "# - valid_ratio が適度にある → assistant部分にしっかりlossが乗っている\n",
        "#\n",
        "# 初学者向けに言うと：\n",
        "# - これは“学習がちゃんと効いているかの健康診断”です。\n",
        "\n",
        "class LabelStatsCallback(TrainerCallback):\n",
        "    def __init__(self, dataset, collator, name=\"train\", every_n_steps=100):\n",
        "        self.dataset, self.collator, self.name, self.every_n_steps = dataset, collator, name, every_n_steps\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if (state.global_step % self.every_n_steps) == 0:\n",
        "            batch = [self.dataset[random.randint(0, len(self.dataset)-1)] for _ in range(8)]\n",
        "            out = self.collator(batch)\n",
        "            valid = (out[\"labels\"] != -100).sum().item()\n",
        "            total = (out[\"attention_mask\"] == 1).sum().item()\n",
        "            print(f\"\\n[LabelStats:{self.name}] step={state.global_step} valid_ratio={valid/max(1,total):.4f}\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2.6) Main\n",
        "# -----------------------------\n",
        "# 学習を実行します。\n",
        "\n",
        "def main():\n",
        "    os.makedirs(OUT_LORA_DIR, exist_ok=True)\n",
        "\n",
        "    # if you used /content/your_id cache dirs etc, remove to avoid confusion\n",
        "    if os.path.exists(\"/content/your_id\"):\n",
        "        shutil.rmtree(\"/content/your_id\")\n",
        "\n",
        "    print(f\"[INFO] Loading dataset from HF Hub: {DATASET_ID}\")\n",
        "    ds_all = load_dataset(DATASET_ID, split=\"train\")\n",
        "\n",
        "    # データ形式チェック（messagesがlistであること）\n",
        "    ensure_openai_messages(ds_all)\n",
        "\n",
        "    # 学習できるサンプルだけ残す（assistantが空なら教師信号が無い）\n",
        "    ds_all = ds_all.filter(lambda ex: has_any_nonempty_assistant_turn(ex[\"messages\"]))\n",
        "    ds_all = ds_all.filter(ends_with_nonempty_assistant)\n",
        "\n",
        "    # train/val分割\n",
        "    train_ds, val_ds = shuffle_split(ds_all, VAL_RATIO, SEED)\n",
        "\n",
        "    # Optional: upsampling by rule（分割後に適用）\n",
        "    train_ds = apply_upsampling(train_ds)\n",
        "\n",
        "    print(\"[INFO] Loading base model:\", BASE_MODEL_ID)\n",
        "\n",
        "    # Unslothでベースモデルを読み込む（4bitロードで省メモリ）\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=BASE_MODEL_ID,\n",
        "        max_seq_length=MAX_SEQ_LEN,\n",
        "        dtype=None,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "\n",
        "    # Cache chat template renders（tokenizerが必要なのでここで初めてbuild_cacheを作る）\n",
        "    build_cache = make_text_cache_builder(tokenizer)\n",
        "\n",
        "    train_ds = train_ds.map(build_cache, batched=True, num_proc=1, desc=\"Caching train\")\n",
        "    val_ds   = val_ds.map(build_cache,   batched=True, num_proc=1, desc=\"Caching val\")\n",
        "\n",
        "    # Attach LoRA\n",
        "    # ここで「学習される部分（LoRAアダプタ）」をモデルに追加します。\n",
        "    # 学習対象は LoRA のパラメータだけになり、ベースモデルの巨大な重みは固定されます。\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=LORA_R,\n",
        "        target_modules=LORA_TARGET_MODULES,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "        random_state=SEED,\n",
        "    )\n",
        "\n",
        "\n",
        "    # Transformersの引数名がバージョンで揺れることがあります。\n",
        "    # 今回のバージョンでは eval_strategy を使います。\n",
        "    args = TrainingArguments(\n",
        "        output_dir=OUT_LORA_DIR,\n",
        "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
        "        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
        "        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        learning_rate=LR,\n",
        "        warmup_ratio=WARMUP_RATIO,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "\n",
        "        logging_steps=LOGGING_STEPS,\n",
        "\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=EVAL_STEPS,\n",
        "\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=SAVE_STEPS,\n",
        "        save_total_limit=SAVE_TOTAL_LIMIT,\n",
        "\n",
        "        max_steps=MAX_STEPS,  # -1 => epoch-based\n",
        "\n",
        "        bf16=False,\n",
        "        fp16=True,            # T4向け（T4はbf16が弱いのでfp16を使うのが一般的）\n",
        "\n",
        "        push_to_hub=False,\n",
        "        report_to=\"none\",\n",
        "\n",
        "        group_by_length=False,\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "    # assistant-only loss の collator を使う\n",
        "    collator = AssistantOnlyCollatorCached(tokenizer=tokenizer, max_length=MAX_SEQ_LEN)\n",
        "\n",
        "    # --- NaN対策：all-masked（教師トークン0）を除去して評価を安定化 ---\n",
        "    print(\"[INFO] Checking all-masked samples before filtering...\")\n",
        "    count_all_masked(val_ds, collator, n=len(val_ds), seed=SEED)\n",
        "\n",
        "    print(\"[INFO] Filtering train/val to remove all-masked samples...\")\n",
        "    train_ds = filter_has_supervision(train_ds, collator)\n",
        "    val_ds   = filter_has_supervision(val_ds, collator)\n",
        "\n",
        "    print(\"[INFO] New sizes:\", \"train =\", len(train_ds), \"val =\", len(val_ds))\n",
        "    print(\"[INFO] Checking all-masked samples after filtering...\")\n",
        "    count_all_masked(val_ds, collator, n=len(val_ds), seed=SEED)\n",
        "\n",
        "\n",
        "    # Trainer（Transformersの標準学習ループ）\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        data_collator=collator,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    # 監視用コールバックを追加（学習が効いているかのヘルスチェック）\n",
        "    trainer.add_callback(LabelStatsCallback(train_ds, collator, name=\"train\", every_n_steps=LOGGING_STEPS))\n",
        "\n",
        "    print(\"[INFO] Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # 学習後の保存：LoRAアダプタ＆tokenizer\n",
        "    print(\"[INFO] Saving adapter & tokenizer...\")\n",
        "    model.save_pretrained(OUT_LORA_DIR)\n",
        "    tokenizer.save_pretrained(OUT_LORA_DIR)\n",
        "    print(f\"[INFO] Done. Saved to {OUT_LORA_DIR}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: 学習成果物の確認と、LoRAアダプタのhuggingfaceへのアップロード\n",
        "学習後、OUT_LORA_DIR に以下が保存されます（最低限）ので、確認してください。\n",
        "- adapter_config.json\n",
        "- adapter_model.safetensors（または adapter_model.bin）\n",
        "- tokenizer 関連ファイル\n",
        "<br>\n",
        "\n",
        "下記に従って\"README.md\"を記載してから、HuggingFaceにアダプタアップロードを実行してください。"
      ],
      "metadata": {
        "id": "nVAg0vIdVLg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### ① README.md の正しい書き方\n",
        "\n",
        "#### README.md の役割（最重要）\n",
        "\n",
        "Hugging Face では **README.md = モデルカード**です。\n",
        "「このLoRAは何を学習し、どう使い、何に注意すべきか」を **第三者が再利用できる水準で説明する義務**があります。\n",
        "\n",
        "README が不十分なモデルは、\n",
        "\n",
        "* OSSとして不適切\n",
        "* 学習内容が不透明\n",
        "* ライセンス違反リスクあり\n",
        "  と評価されます。\n",
        "\n",
        "---\n",
        "\n",
        "#### 必須構成（この順で書くこと）\n",
        "\n",
        "##### 1. YAMLメタデータ（必須）\n",
        "\n",
        "```yaml\n",
        "---\n",
        "base_model: Qwen/Qwen3-4B-Instruct-2507\n",
        "datasets:\n",
        "- u-10bei/structured_data_with_cot_dataset_512_v2\n",
        "language:\n",
        "- en\n",
        "license: Apache-2.0\n",
        "library_name: peft\n",
        "pipeline_tag: text-generation\n",
        "tags:\n",
        "- qlora\n",
        "- lora\n",
        "- structured-output\n",
        "---\n",
        "```\n",
        "\n",
        "**理由**\n",
        "\n",
        "* HF検索・分類・再現性に必須\n",
        "* 無いと「壊れたモデルカード」扱いになる\n",
        "\n",
        "---\n",
        "\n",
        "##### 2. モデル概要（What）\n",
        "\n",
        "```md\n",
        "# qwen3-4b-structured-output-lora\n",
        "\n",
        "This repository provides a **LoRA adapter** fine-tuned from\n",
        "**Qwen3-4B-Instruct-2507** using **QLoRA (4-bit, Unsloth)**.\n",
        "\n",
        "This repository contains **LoRA adapter weights only**.\n",
        "The base model must be loaded separately.\n",
        "```\n",
        "\n",
        "**必須ポイント**\n",
        "\n",
        "* 「LoRAアダプタのみ」であることを明記\n",
        "* ベースモデル名を明示\n",
        "\n",
        "---\n",
        "\n",
        "##### 3. 学習目的・設計思想（Why）\n",
        "\n",
        "```md\n",
        "## Training Objective\n",
        "\n",
        "This adapter is trained to improve **structured output accuracy**\n",
        "(JSON / YAML / XML / TOML / CSV).\n",
        "\n",
        "Loss is applied only to the final assistant output,\n",
        "while intermediate reasoning (Chain-of-Thought) is masked.\n",
        "```\n",
        "\n",
        "**今回の講座では特に重要**\n",
        "\n",
        "* assistant-only loss\n",
        "* CoT mask（Output: 以降のみ学習）\n",
        "\n",
        "---\n",
        "\n",
        "##### 4. 学習設定（How）\n",
        "\n",
        "```md\n",
        "## Training Configuration\n",
        "\n",
        "- Base model: Qwen3-4B-Instruct-2507\n",
        "- Method: QLoRA (4-bit)\n",
        "- Max sequence length: 512\n",
        "- Epochs: 1\n",
        "- Learning rate: 1e-6\n",
        "- LoRA: r=64, alpha=128\n",
        "```\n",
        "\n",
        "**再現性のため必須**\n",
        "\n",
        "---\n",
        "\n",
        "##### 5. 使用方法（How to use）\n",
        "\n",
        "````md\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "base = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "adapter = \"your_id/your-repo\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, adapter)\n",
        "````\n",
        "\n",
        "````\n",
        "\n",
        "---\n",
        "\n",
        "##### 6. データセット・ライセンス注意（必須・重要）\n",
        "```md\n",
        "## Sources & Terms (IMPORTANT)\n",
        "\n",
        "Training data: u-10bei/structured_data_with_cot_dataset_512_v2\n",
        "\n",
        "Dataset License: MIT License. This dataset is used and distributed under the terms of the MIT License.\n",
        "Compliance: Users must comply with the MIT license (including copyright notice) and the base model's original terms of use.\n",
        "````\n",
        "---\n",
        "##### 実行コードの見本\n",
        "\n",
        "- 【課題】最低限、モデルタイトルの欄は、必ず自身で書き込んで下さい。\n",
        "- 使用データセットを変更した場合には、\"Dataset License\",\"Compliance\" 欄も適切な形に書き換えてください。"
      ],
      "metadata": {
        "id": "t0AyjusDD2od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# README.md（モデルカード）を OUT_LORA_DIR に生成\n",
        "# -----------------------------\n",
        "# 学習完了後に実行し、Hugging Face の README.md（モデルカード）を生成\n",
        "# ベースモデル名・データセット名・学習ハイパーパラメータはコードの変数から自動同期\n",
        "\n",
        "import os\n",
        "\n",
        "os.makedirs(OUT_LORA_DIR, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 補助関数の定義\n",
        "# ------------------------------------------------------------------\n",
        "def _s(x, default=\"\"):\n",
        "    try:\n",
        "        v = str(x)\n",
        "        return v if v.strip() else default\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "def _fmt_lr(x) -> str:\n",
        "    \"\"\"\n",
        "    Learning Rate の表記を整えるための関数。\n",
        "\n",
        "    - 数値として解釈できる場合：\n",
        "      指数表記（例: 1e-6）に整形する\n",
        "    - 数値として解釈できない場合：\n",
        "      元の値をそのまま文字列として出力する\n",
        "      （誤った値を生成しないための安全策）\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return f\"{float(x):.0e}\"\n",
        "    except Exception:\n",
        "        return _s(x, \"\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 学習コードの変数から値を取得（README と自動同期）\n",
        "# ------------------------------------------------------------------\n",
        "base_model_id = _s(BASE_MODEL_ID, \"Qwen/Qwen3-4B-Instruct-2507\")\n",
        "dataset_id = _s(DATASET_ID, \"https://huggingface.co/datasets/u-10bei/structured_data_with_cot_dataset_512_v2\")\n",
        "\n",
        "max_seq_len = int(MAX_SEQ_LEN)\n",
        "epochs = int(NUM_TRAIN_EPOCHS)\n",
        "lr_str = _fmt_lr(LR)\n",
        "\n",
        "lora_r = int(LORA_R)\n",
        "lora_alpha = int(LORA_ALPHA)\n",
        "\n",
        "# NOTE:\n",
        "# - YAML front matter の license は\n",
        "#   「この LoRA アダプタ（リポジトリ）のライセンス表明」を意味する。\n",
        "# - 必要に応じて環境変数で差し替え可能。\n",
        "repo_license = os.environ.get(\"SFT_REPO_LICENSE\", \"apache-2.0\")\n",
        "\n",
        "# README 内に記載するモデルタイトル\n",
        "# 変更したい場合は README.md を手書きで調整\n",
        "model_title = \"Qwen3-4B-Instruct-2507-lora-rev.02\"\n",
        "title_line = f\"Qwen/{model_title}\" #例： qwen3-4b-structured-output-lora\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# README.md 本文の生成\n",
        "# （説明テキストに準拠し、変数部分のみを自動置換）\n",
        "# ------------------------------------------------------------------\n",
        "readme_md = f\"\"\"---\n",
        "base_model: {base_model_id}\n",
        "datasets:\n",
        "- {dataset_id}\n",
        "language:\n",
        "- en\n",
        "license: {repo_license}\n",
        "library_name: peft\n",
        "pipeline_tag: text-generation\n",
        "tags:\n",
        "- qlora\n",
        "- lora\n",
        "- structured-output\n",
        "---\n",
        "\n",
        "{title_line}\n",
        "\n",
        "This repository provides a **LoRA adapter** fine-tuned from\n",
        "**{base_model_id}** using **QLoRA (4-bit, Unsloth)**.\n",
        "\n",
        "This repository contains **LoRA adapter weights only**.\n",
        "The base model must be loaded separately.\n",
        "\n",
        "## Training Objective\n",
        "\n",
        "This adapter is trained to improve **structured output accuracy**\n",
        "(JSON / YAML / XML / TOML / CSV).\n",
        "\n",
        "Loss is applied only to the final assistant output,\n",
        "while intermediate reasoning (Chain-of-Thought) is masked.\n",
        "\n",
        "## Training Configuration\n",
        "\n",
        "- Base model: {base_model_id}\n",
        "- Method: QLoRA (4-bit)\n",
        "- Max sequence length: {max_seq_len}\n",
        "- Epochs: {epochs}\n",
        "- Learning rate: {lr_str}\n",
        "- LoRA: r={lora_r}, alpha={lora_alpha}\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "base = \"{base_model_id}\"\n",
        "adapter = \"your_id/your-repo\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, adapter)\n",
        "```\n",
        "\n",
        "## Sources & Terms (IMPORTANT)\n",
        "\n",
        "Training data: {dataset_id}\n",
        "\n",
        "Dataset License: MIT License. This dataset is used and distributed under the terms of the MIT License.\n",
        "Compliance: Users must comply with the MIT license (including copyright notice) and the base model's original terms of use.\n",
        "\"\"\"\n",
        "# ------------------------------------------------------------------\n",
        "# README.md の書き込み\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "readme_path = os.path.join(OUT_LORA_DIR, \"README.md\")\n",
        "with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(readme_md)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 動作確認\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "assert os.path.exists(readme_path), \"README.md was not written.\"\n",
        "assert readme_md.lstrip().startswith(\"---\\n\"), (\n",
        "    \"README.md must start with YAML front matter.\"\n",
        ")\n",
        "# 修正: 先頭の --- は改行なしで始まるため count(\"\\n---\\n\") には含まれない。\n",
        "# そのため、閉じタグの分として 1回以上あればOKとする。\n",
        "assert readme_md.count(\"\\n---\\n\") >= 1, (\n",
        "    \"YAML front matter must be closed properly.\"\n",
        ")\n",
        "\n",
        "print(f\"[INFO] README.md written to: {readme_path}\")\n",
        "print(\"[INFO] Preview (first 30 lines):\")\n",
        "for i, line in enumerate(readme_md.splitlines()[:30], start=1):\n",
        "    print(f\"{i:02d}: {line}\")"
      ],
      "metadata": {
        "id": "FhZ-2hZBSSSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### ② README.md の HF アップロードコード\n",
        "\n",
        "以下は **README.md を自動生成しません**。\n",
        "\n",
        "* 直前のコードを参照してREADME.md 完成させ、 OUT_LORA_DIR に保存してから実行してください。\n",
        "* アップロード対象として README.md を必須化\n",
        "* README.md が存在しない場合 → **エラー**\n",
        "---"
      ],
      "metadata": {
        "id": "1yne5up9W4OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 3) LoRAアダプターをHugging Faceへアップロード (作成済みのREADMEを含む)\n",
        "# ============================================================\n",
        "\n",
        "import fnmatch\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Hugging Face APIの操作用インスタンスを作成\n",
        "api = HfApi()\n",
        "\n",
        "# 各種パスや設定の準備\n",
        "LORA_SAVE_DIR = Path(OUT_LORA_DIR)  # 学習済みモデルが保存されているディレクトリ\n",
        "HF_REPO_ID    = _getenv(\"HF_REPO_ID\", f\"keeponing/{model_title}\")  # アップロード先のレポジトリID\n",
        "\n",
        "# 非公開設定の確認（環境変数が '1' または 'true' ならプライベート設定にする）\n",
        "PRIVATE       = _getenv(\"HF_PRIVATE\", \"1\") in (\"1\",\"true\",\"True\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3.1) 必須ファイルの存在確認\n",
        "# -----------------------------\n",
        "# アップロードに最低限必要なファイルを定義します\n",
        "required_files = {\n",
        "    \"adapter_config.json\", # LoRAの設定ファイル\n",
        "    \"README.md\",           # 受講生が作成した解説文書\n",
        "}\n",
        "\n",
        "# 保存ディレクトリにあるファイル名のリストを取得\n",
        "present = {p.name for p in LORA_SAVE_DIR.iterdir() if p.is_file()}\n",
        "\n",
        "# 足りないファイルをリストアップ\n",
        "missing = [f for f in required_files if f not in present]\n",
        "\n",
        "# モデル本体（adapter_model.safetensors または .bin）が存在するか確認\n",
        "if not any(f.startswith(\"adapter_model.\") for f in present):\n",
        "    missing.append(\"adapter_model.(safetensors|bin)\")\n",
        "\n",
        "# 必須ファイルが欠けている場合は、エラーを表示して処理を中断します\n",
        "if missing:\n",
        "    raise RuntimeError(\n",
        "        \"アップロードを中止しました。\\n\"\n",
        "        \"以下の必須ファイルが見つかりません:\\n\"\n",
        "        + \"\\n\".join(f\"- {m}\" for m in missing) +\n",
        "        \"\\n\\nアップロード前に、README.md を手書きで作成し保存してください。\"\n",
        "    )\n",
        "\n",
        "print(\"✅ 必須ファイルの確認が完了しました。\")\n"
      ],
      "metadata": {
        "id": "7bLNMxfZOwIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# 3.2) アップロード対象の選別（ホワイトリスト）\n",
        "# -----------------------------\n",
        "# 不要な一時ファイルなどをアップロードしないよう、許可するファイル形式を指定します\n",
        "ALLOW_PATTERNS = [\n",
        "    \"README.md\",\n",
        "    \"adapter_config.json\",\n",
        "    \"adapter_model.*\",\n",
        "    \"tokenizer.*\",\n",
        "    \"special_tokens_map.json\",\n",
        "    \"*.json\",\n",
        "]\n",
        "\n",
        "def is_allowed(name: str) -> bool:\n",
        "    \"\"\"ファイル名が許可パターンに一致するか判定する関数\"\"\"\n",
        "    return any(fnmatch.fnmatch(name, pat) for pat in ALLOW_PATTERNS)\n",
        "\n",
        "# アップロード用の一時フォルダ（ステージング領域）を作成\n",
        "STAGE_DIR = Path(\"/content/hf_upload_stage\")\n",
        "\n",
        "if STAGE_DIR.exists():\n",
        "    shutil.rmtree(STAGE_DIR) # 既存のフォルダがあれば一旦削除\n",
        "STAGE_DIR.mkdir(parents=True)\n",
        "\n",
        "# 許可されたファイルだけを一時フォルダにコピー\n",
        "for p in LORA_SAVE_DIR.iterdir():\n",
        "    if p.is_file() and is_allowed(p.name):\n",
        "        (STAGE_DIR / p.name).write_bytes(p.read_bytes())\n",
        "\n",
        "print(\"📦 アップロード対象ファイル:\", [p.name for p in STAGE_DIR.iterdir()])\n"
      ],
      "metadata": {
        "id": "emtBfTIWZ3Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# 3.3) リポジトリ作成とアップロード\n",
        "# -----------------------------\n",
        "\n",
        "# Hugging Face上にリポジトリを作成（既に存在していてもOK）\n",
        "api.create_repo(\n",
        "    repo_id=HF_REPO_ID,\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        "    private=PRIVATE,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "5DO03rB6Z9y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 一時フォルダの内容をまるごとアップロード\n",
        "api.upload_folder(\n",
        "    folder_path=str(STAGE_DIR),\n",
        "    repo_id=HF_REPO_ID,\n",
        "    repo_type=\"model\",\n",
        "    commit_message=\"Upload LoRA adapter (README written by author)\",\n",
        ")\n",
        "\n",
        "print(\"✅ アップロードが正常に完了しました。\")\n",
        "print(f\"URL: https://huggingface.co/{HF_REPO_ID}\")"
      ],
      "metadata": {
        "id": "1twJV9qZaCUa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}